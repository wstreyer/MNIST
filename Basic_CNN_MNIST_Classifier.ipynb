{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from tqdm.autonotebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from numpy import log\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define nn model\n",
    "class Basic_CNN_MNISTClassifer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Basic_CNN_MNISTClassifer, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)  # 4*4 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.output = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Convolution layer C1: 1 input image channel, 6 output channels,\n",
    "        # 5x5 square convolution, it uses RELU activation function, and\n",
    "        # outputs a Tensor with size (N, 6, 28, 28), where N is the size of the batch\n",
    "        c1 = F.relu(self.conv1(input))\n",
    "        # Subsampling layer S2: 2x2 grid, purely functional,\n",
    "        # this layer does not have any parameter, and outputs a (N, 6, 14, 14) Tensor\n",
    "        s2 = F.max_pool2d(c1, (2, 2))\n",
    "        # Convolution layer C3: 6 input channels, 16 output channels,\n",
    "        # 5x5 square convolution, it uses RELU activation function, and\n",
    "        # outputs a (N, 16, 10, 10) Tensor\n",
    "        c3 = F.relu(self.conv2(s2))\n",
    "        # Subsampling layer S4: 2x2 grid, purely functional,\n",
    "        # this layer does not have any parameter, and outputs a (N, 16, 5, 5) Tensor\n",
    "        s4 = F.max_pool2d(c3, 2)\n",
    "        # Flatten operation: purely functional, outputs a (N, 400) Tensor\n",
    "        s4 = torch.flatten(s4, 1)\n",
    "        # Fully connected layer F5: (N, 400) Tensor input,\n",
    "        # and outputs a (N, 120) Tensor, it uses RELU activation function\n",
    "        f5 = F.relu(self.fc1(s4))\n",
    "        # Fully connected layer F6: (N, 120) Tensor input,\n",
    "        # and outputs a (N, 84) Tensor, it uses RELU activation function\n",
    "        f6 = F.relu(self.fc2(f5))\n",
    "        # Gaussian layer OUTPUT: (N, 84) Tensor input, and\n",
    "        # outputs a (N, 10) Tensor\n",
    "        output = self.fc3(f6)\n",
    "        # Output layer\n",
    "        output = self.output(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Source\n",
    "https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n",
    "\n",
    "- Modified to accept single-channel 28x28 images instead of 3-channel 32x32\n",
    "- Added final output layer of LogSoftMax to assign likelihood value to each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72cd2891966c4b7e8a9de036eace2f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367a2775decd4ab0be086f65c9b763c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5890d2d6efeb43f1b33f0525589aac40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load, transform, and split, and pre-cache data\n",
    "# Load data\n",
    "batch_size = 1000\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Training data\n",
    "train_data = MNIST(os.getcwd(), train=True, download=True, transform=transform)\n",
    "train_data, val_data = random_split(train_data, [0.9, 0.1])\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=os.cpu_count())\n",
    "train_loader = [itm for itm in tqdm(train_loader)]\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, num_workers=os.cpu_count())\n",
    "val_loader = [itm for itm in tqdm(val_loader)]\n",
    "\n",
    "# Testing data\n",
    "test_data = MNIST(os.getcwd(), train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=os.cpu_count())\n",
    "test_loader = [itm for itm in tqdm(test_loader)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAZf0lEQVR4nO3df2zU953n8ddgYGK48VxdYs84OJbVg7bClFWBAl5+GFQs5lQ2xOmKJHc9o2tR0hgk1omiEk7C6h84IgeHtG7oNtdSUKGg1RKCDhTiHtiUpfQMSy6sG3HOYYIrPGvhSzzGIWMMn/uDYy6Djcl3mOHtsZ8P6Svhme+HefPNN3nmy3i+9jnnnAAAMDDOegAAwNhFhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgJnx1gPc6/bt27p69aoCgYB8Pp/1OAAAj5xz6u3tVVFRkcaNG/5aZ8RF6OrVqyouLrYeAwDwkDo6OjR16tRh9xlxEQoEApKkhfq3Gq8JxtMAALwa0E2d0tHEf8+Hk7EIvfnmm3rjjTfU2dmpGTNmaMeOHVq0aNED1939K7jxmqDxPiIEAFnn/92R9Mu8pZKRb0w4cOCANmzYoE2bNun8+fNatGiRIpGIrly5komXAwBkqYxEaPv27frhD3+oH/3oR/rmN7+pHTt2qLi4WDt37szEywEAslTaI9Tf369z586psrIy6fHKykqdPn160P7xeFyxWCxpAwCMDWmP0LVr13Tr1i0VFhYmPV5YWKhoNDpo//r6egWDwcTGd8YBwNiRsQ+r3vuGlHNuyDepNm7cqJ6ensTW0dGRqZEAACNM2r87bsqUKcrJyRl01dPV1TXo6kiS/H6//H5/uscAAGSBtF8JTZw4UbNnz1ZjY2PS442NjSovL0/3ywEAslhGPidUW1urH/zgB5ozZ44WLFigX/ziF7py5YpefPHFTLwcACBLZSRCq1evVnd3t37605+qs7NTZWVlOnr0qEpKSjLxcgCALOVzzjnrIb4oFospGAyqQk9xxwQAyEID7qaa9I56enqUl5c37L78KAcAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADAzHjrAQCMPG7BLM9r3vuH3Z7XHO6b5HnNm88+7XmNO9fqeQ0eDa6EAABmiBAAwEzaI1RXVyefz5e0hUKhdL8MAGAUyMh7QjNmzNDvfve7xNc5OTmZeBkAQJbLSITGjx/P1Q8A4IEy8p5QW1ubioqKVFpaqmeffVaXLl26777xeFyxWCxpAwCMDWmP0Lx587Rnzx4dO3ZMb731lqLRqMrLy9Xd3T3k/vX19QoGg4mtuLg43SMBAEaotEcoEonomWee0cyZM/Xd735XR44ckSTt3j30Zwg2btyonp6exNbR0ZHukQAAI1TGP6w6efJkzZw5U21tbUM+7/f75ff7Mz0GAGAEyvjnhOLxuD788EOFw+FMvxQAIMukPUKvvPKKmpub1d7erj/+8Y/6/ve/r1gspurq6nS/FAAgy6X9r+P+/Oc/67nnntO1a9f0+OOPa/78+Tpz5oxKSkrS/VIAgCyX9gjt378/3b8lgBTl/JvSlNZV/tdmz2tuulue10Qm9Xpe8/lv/5vnNb96fqXnNZLkzv5zSuvw5XHvOACAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADATMZ/qB2ANBmX43nJla2TUnqpmn/9v1Na9yg8Pfn/eF7zRl1/Sq+V/72UlsEDroQAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghrtoA1li3Le+7nnN+Xl7MjBJ9rl15KvWI+A+uBICAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMxwA1MgS/z7A8esRxgRXvuXOZ7XhP7+f6X0WrdSWgUvuBICAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMxwA1PAwEf/Zb7nNc/+q3/yvOa25xWPVmv/gOc1/7h1nuc1gWtnPK/Bo8GVEADADBECAJjxHKGTJ09q5cqVKioqks/n06FDh5Ked86prq5ORUVFys3NVUVFhVpbW9M1LwBgFPEcob6+Ps2aNUsNDQ1DPr9161Zt375dDQ0NamlpUSgU0vLly9Xb2/vQwwIARhfP35gQiUQUiUSGfM45px07dmjTpk2qqqqSJO3evVuFhYXat2+fXnjhhYebFgAwqqT1PaH29nZFo1FVVlYmHvP7/VqyZIlOnz495Jp4PK5YLJa0AQDGhrRGKBqNSpIKCwuTHi8sLEw8d6/6+noFg8HEVlxcnM6RAAAjWEa+O87n8yV97Zwb9NhdGzduVE9PT2Lr6OjIxEgAgBEorR9WDYVCku5cEYXD4cTjXV1dg66O7vL7/fL7/ekcAwCQJdJ6JVRaWqpQKKTGxsbEY/39/WpublZ5eXk6XwoAMAp4vhK6fv26Pvroo8TX7e3tev/995Wfn68nn3xSGzZs0JYtWzRt2jRNmzZNW7Zs0aRJk/T888+ndXAAQPbzHKGzZ89q6dKlia9ra2slSdXV1fr1r3+tV199VTdu3NBLL72kTz75RPPmzdN7772nQCCQvqkBAKOCzznnrIf4olgspmAwqAo9pfG+CdbjAA/U9ZL3v2r+x9d2eF7j93l/C/e2RtS/3oPMeWO95zWhHUN/3AMjx4C7qSa9o56eHuXl5Q27L/eOAwCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgJm0/mRVYCz6fIr3NRN8OekfxNh3zv47z2ue+NU/e15zy/MKjGRcCQEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZriBKfBF87/leUlD9d9lYBBb5eef87wm9HyH5zW3+vo8r8HowpUQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGG5gCX9C+wfuaxY/1p32OdKm48NcprfvqMx97XnM7Hk/ptTC2cSUEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJjhBqYYldxf/kVK696ev9PzmnHyp/RaXl133m8Q+tgbX0nptVz8UkrrAK+4EgIAmCFCAAAzniN08uRJrVy5UkVFRfL5fDp06FDS82vWrJHP50va5s+fn655AQCjiOcI9fX1adasWWpoaLjvPitWrFBnZ2diO3r06EMNCQAYnTx/Y0IkElEkEhl2H7/fr1AolPJQAICxISPvCTU1NamgoEDTp0/X2rVr1dXVdd994/G4YrFY0gYAGBvSHqFIJKK9e/fq+PHj2rZtm1paWrRs2TLF7/Pz5+vr6xUMBhNbcXFxukcCAIxQaf+c0OrVqxO/Lisr05w5c1RSUqIjR46oqqpq0P4bN25UbW1t4utYLEaIAGCMyPiHVcPhsEpKStTW1jbk836/X37/o/mwHwBgZMn454S6u7vV0dGhcDic6ZcCAGQZz1dC169f10cffZT4ur29Xe+//77y8/OVn5+vuro6PfPMMwqHw7p8+bJee+01TZkyRU8//XRaBwcAZD/PETp79qyWLl2a+Pru+znV1dXauXOnLly4oD179ujTTz9VOBzW0qVLdeDAAQUCgfRNDQAYFTxHqKKiQs65+z5/7NixhxoIuNe4SZM8r/n4bwZSeq3pEyZ6XnNb9//3IZ3+8n/8yPOaJ/77uQxMAqQP944DAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAmYz/ZFXgYUX/4194XvM/F/xt+gdJo9Z+73f5fqKqNQOTALa4EgIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzHADUzxS42Z90/Oav1rbnIFJbD236288r3lSpzMwCWCLKyEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAw3MEXKcr7yFc9rpv+qzfOa/zTlA89rHqXlrc94XlNSf9bzGud5BTDycSUEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJjhBqZIWf+sUs9r3gi9lYFJbP3LqSLPa568eTn9gwBZiCshAIAZIgQAMOMpQvX19Zo7d64CgYAKCgq0atUqXbx4MWkf55zq6upUVFSk3NxcVVRUqLW1Na1DAwBGB08Ram5uVk1Njc6cOaPGxkYNDAyosrJSfX19iX22bt2q7du3q6GhQS0tLQqFQlq+fLl6e3vTPjwAILt5+saEd999N+nrXbt2qaCgQOfOndPixYvlnNOOHTu0adMmVVVVSZJ2796twsJC7du3Ty+88EL6JgcAZL2Hek+op6dHkpSfny9Jam9vVzQaVWVlZWIfv9+vJUuW6PTp00P+HvF4XLFYLGkDAIwNKUfIOafa2lotXLhQZWVlkqRoNCpJKiwsTNq3sLAw8dy96uvrFQwGE1txcXGqIwEAskzKEVq3bp0++OAD/fa3vx30nM/nS/raOTfosbs2btyonp6exNbR0ZHqSACALJPSh1XXr1+vw4cP6+TJk5o6dWri8VAoJOnOFVE4HE483tXVNejq6C6/3y+/35/KGACALOfpSsg5p3Xr1ungwYM6fvy4SkuTPzFfWlqqUCikxsbGxGP9/f1qbm5WeXl5eiYGAIwanq6EampqtG/fPr3zzjsKBAKJ93mCwaByc3Pl8/m0YcMGbdmyRdOmTdO0adO0ZcsWTZo0Sc8//3xG/gAAgOzlKUI7d+6UJFVUVCQ9vmvXLq1Zs0aS9Oqrr+rGjRt66aWX9Mknn2jevHl67733FAgE0jIwAGD08DnnnPUQXxSLxRQMBlWhpzTeN8F6HAyjbfe3Pa+5+N2RfQPTrx96yfOa6bXve17j4nHPa4BsMeBuqknvqKenR3l5ecPuy73jAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYCaln6yK0SVnyldTWvfX3/qnNE9i7+tv9Xpec5s7YgMp40oIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADDDDUwhX25uSuumPxZN8yTpsyf2RErrxnXHPK+5ndIrAZC4EgIAGCJCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzHADU2ig488prfvPB6o8r/kPa//W85q9vWHPa/6hapHnNZJ0q6MtpXUAUsOVEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABgxuecc9ZDfFEsFlMwGFSFntJ43wTrcQAAHg24m2rSO+rp6VFeXt6w+3IlBAAwQ4QAAGY8Rai+vl5z585VIBBQQUGBVq1apYsXLybts2bNGvl8vqRt/vz5aR0aADA6eIpQc3OzampqdObMGTU2NmpgYECVlZXq6+tL2m/FihXq7OxMbEePHk3r0ACA0cHTT1Z99913k77etWuXCgoKdO7cOS1evDjxuN/vVygUSs+EAIBR66HeE+rp6ZEk5efnJz3e1NSkgoICTZ8+XWvXrlVXV9d9f494PK5YLJa0AQDGhpQj5JxTbW2tFi5cqLKyssTjkUhEe/fu1fHjx7Vt2za1tLRo2bJlisfjQ/4+9fX1CgaDia24uDjVkQAAWSblzwnV1NToyJEjOnXqlKZOnXrf/To7O1VSUqL9+/erqqpq0PPxeDwpULFYTMXFxXxOCACylJfPCXl6T+iu9evX6/Dhwzp58uSwAZKkcDiskpIStbW1Dfm83++X3+9PZQwAQJbzFCHnnNavX6+3335bTU1NKi0tfeCa7u5udXR0KBwOpzwkAGB08vSeUE1NjX7zm99o3759CgQCikajikajunHjhiTp+vXreuWVV/SHP/xBly9fVlNTk1auXKkpU6bo6aefzsgfAACQvTxdCe3cuVOSVFFRkfT4rl27tGbNGuXk5OjChQvas2ePPv30U4XDYS1dulQHDhxQIBBI29AAgNHB81/HDSc3N1fHjh17qIEAAGMH944DAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJgZbz3AvZxzkqQB3ZSc8TAAAM8GdFPS///v+XBGXIR6e3slSad01HgSAMDD6O3tVTAYHHYfn/syqXqEbt++ratXryoQCMjn8yU9F4vFVFxcrI6ODuXl5RlNaI/jcAfH4Q6Owx0chztGwnFwzqm3t1dFRUUaN274d31G3JXQuHHjNHXq1GH3ycvLG9Mn2V0chzs4DndwHO7gONxhfRwedAV0F9+YAAAwQ4QAAGayKkJ+v1+bN2+W3++3HsUUx+EOjsMdHIc7OA53ZNtxGHHfmAAAGDuy6koIADC6ECEAgBkiBAAwQ4QAAGayKkJvvvmmSktL9dhjj2n27Nn6/e9/bz3SI1VXVyefz5e0hUIh67Ey7uTJk1q5cqWKiork8/l06NChpOedc6qrq1NRUZFyc3NVUVGh1tZWm2Ez6EHHYc2aNYPOj/nz59sMmyH19fWaO3euAoGACgoKtGrVKl28eDFpn7FwPnyZ45At50PWROjAgQPasGGDNm3apPPnz2vRokWKRCK6cuWK9WiP1IwZM9TZ2ZnYLly4YD1SxvX19WnWrFlqaGgY8vmtW7dq+/btamhoUEtLi0KhkJYvX564D+Fo8aDjIEkrVqxIOj+OHh1d92Bsbm5WTU2Nzpw5o8bGRg0MDKiyslJ9fX2JfcbC+fBljoOUJeeDyxLf+c533Isvvpj02De+8Q33k5/8xGiiR2/z5s1u1qxZ1mOYkuTefvvtxNe3b992oVDIvf7664nHPv/8cxcMBt3Pf/5zgwkfjXuPg3POVVdXu6eeespkHitdXV1OkmtubnbOjd3z4d7j4Fz2nA9ZcSXU39+vc+fOqbKyMunxyspKnT592mgqG21tbSoqKlJpaameffZZXbp0yXokU+3t7YpGo0nnht/v15IlS8bcuSFJTU1NKigo0PTp07V27Vp1dXVZj5RRPT09kqT8/HxJY/d8uPc43JUN50NWROjatWu6deuWCgsLkx4vLCxUNBo1murRmzdvnvbs2aNjx47prbfeUjQaVXl5ubq7u61HM3P3n/9YPzckKRKJaO/evTp+/Li2bdumlpYWLVu2TPF43Hq0jHDOqba2VgsXLlRZWZmksXk+DHUcpOw5H0bcXbSHc++PdnDODXpsNItEIolfz5w5UwsWLNDXvvY17d69W7W1tYaT2Rvr54YkrV69OvHrsrIyzZkzRyUlJTpy5IiqqqoMJ8uMdevW6YMPPtCpU6cGPTeWzof7HYdsOR+y4kpoypQpysnJGfR/Ml1dXYP+j2csmTx5smbOnKm2tjbrUczc/e5Azo3BwuGwSkpKRuX5sX79eh0+fFgnTpxI+tEvY+18uN9xGMpIPR+yIkITJ07U7Nmz1djYmPR4Y2OjysvLjaayF4/H9eGHHyocDluPYqa0tFShUCjp3Ojv71dzc/OYPjckqbu7Wx0dHaPq/HDOad26dTp48KCOHz+u0tLSpOfHyvnwoOMwlBF7Phh+U4Qn+/fvdxMmTHC//OUv3Z/+9Ce3YcMGN3nyZHf58mXr0R6Zl19+2TU1NblLly65M2fOuO9973suEAiM+mPQ29vrzp8/786fP+8kue3bt7vz58+7jz/+2Dnn3Ouvv+6CwaA7ePCgu3DhgnvuuedcOBx2sVjMePL0Gu449Pb2updfftmdPn3atbe3uxMnTrgFCxa4J554YlQdhx//+McuGAy6pqYm19nZmdg+++yzxD5j4Xx40HHIpvMhayLknHM/+9nPXElJiZs4caL79re/nfTtiGPB6tWrXTgcdhMmTHBFRUWuqqrKtba2Wo+VcSdOnHCSBm3V1dXOuTvflrt582YXCoWc3+93ixcvdhcuXLAdOgOGOw6fffaZq6ysdI8//ribMGGCe/LJJ111dbW7cuWK9dhpNdSfX5LbtWtXYp+xcD486Dhk0/nAj3IAAJjJiveEAACjExECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABg5v8Cx6mlX5vMNqgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explore images returned by train dataloader iterator\n",
    "# Each item returns list of two objects\n",
    "# 1st object is tensor of batched images\n",
    "# 2nd object is tensor of target labels\n",
    "# batch tensor has shape of (batch_size, channels, width, height)\n",
    "batch_index = torch.randint(len(test_loader), (1,))\n",
    "batch_samples, batch_targets = test_loader[batch_index]\n",
    "sample_index = torch.randint(len(batch_samples), (1,))\n",
    "img_sample = batch_samples[sample_index].squeeze() # remove batch and channel dimensions\n",
    "sample_target = batch_targets[sample_index]\n",
    "plt.imshow(img_sample)\n",
    "print(sample_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd31a6d09ca4462b91d6c140fe197af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0: Training Loss: 2.294, Validation Loss: 2.288, Learing Rate: 1.00E-02\n",
      "Epoch: 1: Training Loss: 2.279, Validation Loss: 2.269, Learing Rate: 1.00E-02\n",
      "Epoch: 2: Training Loss: 2.256, Validation Loss: 2.237, Learing Rate: 1.00E-02\n",
      "Epoch: 3: Training Loss: 2.207, Validation Loss: 2.162, Learing Rate: 1.00E-02\n",
      "Epoch: 4: Training Loss: 2.066, Validation Loss: 1.906, Learing Rate: 1.00E-02\n",
      "Epoch: 5: Training Loss: 1.585, Validation Loss: 1.178, Learing Rate: 1.00E-02\n",
      "Epoch: 6: Training Loss: 0.885, Validation Loss: 0.659, Learing Rate: 1.00E-02\n",
      "Epoch: 7: Training Loss: 0.591, Validation Loss: 0.509, Learing Rate: 1.00E-02\n",
      "Epoch: 8: Training Loss: 0.495, Validation Loss: 0.442, Learing Rate: 1.00E-02\n",
      "Epoch: 9: Training Loss: 0.433, Validation Loss: 0.399, Learing Rate: 1.00E-02\n",
      "Epoch: 10: Training Loss: 0.395, Validation Loss: 0.368, Learing Rate: 1.00E-02\n",
      "Epoch: 11: Training Loss: 0.366, Validation Loss: 0.343, Learing Rate: 1.00E-02\n",
      "Epoch: 12: Training Loss: 0.343, Validation Loss: 0.322, Learing Rate: 1.00E-02\n",
      "Epoch: 13: Training Loss: 0.323, Validation Loss: 0.304, Learing Rate: 1.00E-02\n",
      "Epoch: 14: Training Loss: 0.306, Validation Loss: 0.288, Learing Rate: 1.00E-02\n",
      "Epoch: 15: Training Loss: 0.290, Validation Loss: 0.274, Learing Rate: 1.00E-02\n",
      "Epoch: 16: Training Loss: 0.277, Validation Loss: 0.261, Learing Rate: 1.00E-02\n",
      "Epoch: 17: Training Loss: 0.264, Validation Loss: 0.249, Learing Rate: 1.00E-02\n",
      "Epoch: 18: Training Loss: 0.252, Validation Loss: 0.238, Learing Rate: 1.00E-02\n",
      "Epoch: 19: Training Loss: 0.242, Validation Loss: 0.228, Learing Rate: 1.00E-02\n",
      "Epoch: 20: Training Loss: 0.232, Validation Loss: 0.219, Learing Rate: 1.00E-02\n",
      "Epoch: 21: Training Loss: 0.222, Validation Loss: 0.210, Learing Rate: 1.00E-02\n",
      "Epoch: 22: Training Loss: 0.214, Validation Loss: 0.202, Learing Rate: 1.00E-02\n",
      "Epoch: 23: Training Loss: 0.205, Validation Loss: 0.195, Learing Rate: 1.00E-02\n",
      "Epoch: 24: Training Loss: 0.198, Validation Loss: 0.187, Learing Rate: 1.00E-02\n",
      "Epoch: 25: Training Loss: 0.190, Validation Loss: 0.181, Learing Rate: 1.00E-02\n",
      "Epoch: 26: Training Loss: 0.183, Validation Loss: 0.175, Learing Rate: 1.00E-02\n",
      "Epoch: 27: Training Loss: 0.177, Validation Loss: 0.169, Learing Rate: 1.00E-02\n",
      "Epoch: 28: Training Loss: 0.171, Validation Loss: 0.163, Learing Rate: 1.00E-02\n",
      "Epoch: 29: Training Loss: 0.165, Validation Loss: 0.158, Learing Rate: 1.00E-02\n",
      "Epoch: 30: Training Loss: 0.160, Validation Loss: 0.154, Learing Rate: 1.00E-02\n",
      "Epoch: 31: Training Loss: 0.155, Validation Loss: 0.149, Learing Rate: 1.00E-02\n",
      "Epoch: 32: Training Loss: 0.150, Validation Loss: 0.145, Learing Rate: 1.00E-02\n",
      "Epoch: 33: Training Loss: 0.146, Validation Loss: 0.142, Learing Rate: 1.00E-02\n",
      "Epoch: 34: Training Loss: 0.142, Validation Loss: 0.138, Learing Rate: 1.00E-02\n",
      "Epoch: 35: Training Loss: 0.138, Validation Loss: 0.135, Learing Rate: 1.00E-02\n",
      "Epoch: 36: Training Loss: 0.134, Validation Loss: 0.132, Learing Rate: 1.00E-02\n",
      "Epoch: 37: Training Loss: 0.131, Validation Loss: 0.129, Learing Rate: 1.00E-02\n",
      "Epoch: 38: Training Loss: 0.128, Validation Loss: 0.126, Learing Rate: 1.00E-02\n",
      "Epoch: 39: Training Loss: 0.124, Validation Loss: 0.123, Learing Rate: 1.00E-02\n",
      "Epoch: 40: Training Loss: 0.122, Validation Loss: 0.121, Learing Rate: 1.00E-02\n",
      "Epoch: 41: Training Loss: 0.119, Validation Loss: 0.119, Learing Rate: 1.00E-02\n",
      "Epoch: 42: Training Loss: 0.116, Validation Loss: 0.117, Learing Rate: 1.00E-02\n",
      "Epoch: 43: Training Loss: 0.114, Validation Loss: 0.115, Learing Rate: 1.00E-02\n",
      "Epoch: 44: Training Loss: 0.112, Validation Loss: 0.113, Learing Rate: 1.00E-02\n",
      "Epoch: 45: Training Loss: 0.109, Validation Loss: 0.111, Learing Rate: 1.00E-02\n",
      "Epoch: 46: Training Loss: 0.107, Validation Loss: 0.109, Learing Rate: 1.00E-02\n",
      "Epoch: 47: Training Loss: 0.105, Validation Loss: 0.108, Learing Rate: 1.00E-02\n",
      "Epoch: 48: Training Loss: 0.103, Validation Loss: 0.106, Learing Rate: 1.00E-02\n",
      "Epoch: 49: Training Loss: 0.102, Validation Loss: 0.105, Learing Rate: 1.00E-02\n",
      "Epoch: 50: Training Loss: 0.100, Validation Loss: 0.103, Learing Rate: 1.00E-02\n",
      "Epoch: 51: Training Loss: 0.098, Validation Loss: 0.102, Learing Rate: 1.00E-02\n",
      "Epoch: 52: Training Loss: 0.097, Validation Loss: 0.101, Learing Rate: 1.00E-02\n",
      "Epoch: 53: Training Loss: 0.095, Validation Loss: 0.100, Learing Rate: 1.00E-02\n",
      "Epoch: 54: Training Loss: 0.094, Validation Loss: 0.098, Learing Rate: 1.00E-02\n",
      "Epoch: 55: Training Loss: 0.092, Validation Loss: 0.097, Learing Rate: 1.00E-02\n",
      "Epoch: 56: Training Loss: 0.091, Validation Loss: 0.096, Learing Rate: 1.00E-02\n",
      "Epoch: 57: Training Loss: 0.090, Validation Loss: 0.095, Learing Rate: 1.00E-02\n",
      "Epoch: 58: Training Loss: 0.088, Validation Loss: 0.095, Learing Rate: 1.00E-02\n",
      "Epoch: 59: Training Loss: 0.087, Validation Loss: 0.094, Learing Rate: 1.00E-02\n",
      "Epoch: 60: Training Loss: 0.086, Validation Loss: 0.093, Learing Rate: 1.00E-02\n",
      "Epoch: 61: Training Loss: 0.085, Validation Loss: 0.092, Learing Rate: 1.00E-02\n",
      "Epoch: 62: Training Loss: 0.084, Validation Loss: 0.092, Learing Rate: 1.00E-02\n",
      "Epoch: 63: Training Loss: 0.083, Validation Loss: 0.091, Learing Rate: 1.00E-02\n",
      "Epoch: 64: Training Loss: 0.082, Validation Loss: 0.090, Learing Rate: 1.00E-02\n",
      "Epoch: 65: Training Loss: 0.081, Validation Loss: 0.090, Learing Rate: 1.00E-02\n",
      "Epoch: 66: Training Loss: 0.080, Validation Loss: 0.089, Learing Rate: 1.00E-02\n",
      "Epoch: 67: Training Loss: 0.079, Validation Loss: 0.088, Learing Rate: 1.00E-02\n",
      "Epoch: 68: Training Loss: 0.078, Validation Loss: 0.088, Learing Rate: 1.00E-02\n",
      "Epoch: 69: Training Loss: 0.077, Validation Loss: 0.087, Learing Rate: 1.00E-02\n",
      "Epoch: 70: Training Loss: 0.077, Validation Loss: 0.087, Learing Rate: 1.00E-02\n",
      "Epoch: 71: Training Loss: 0.076, Validation Loss: 0.086, Learing Rate: 1.00E-02\n",
      "Epoch: 72: Training Loss: 0.075, Validation Loss: 0.086, Learing Rate: 1.00E-02\n",
      "Epoch: 73: Training Loss: 0.074, Validation Loss: 0.085, Learing Rate: 1.00E-02\n",
      "Epoch: 74: Training Loss: 0.074, Validation Loss: 0.085, Learing Rate: 1.00E-02\n",
      "Epoch: 75: Training Loss: 0.073, Validation Loss: 0.084, Learing Rate: 1.00E-02\n",
      "Epoch: 76: Training Loss: 0.072, Validation Loss: 0.084, Learing Rate: 1.00E-02\n",
      "Epoch: 77: Training Loss: 0.071, Validation Loss: 0.083, Learing Rate: 1.00E-02\n",
      "Epoch: 78: Training Loss: 0.071, Validation Loss: 0.083, Learing Rate: 1.00E-02\n",
      "Epoch: 79: Training Loss: 0.070, Validation Loss: 0.082, Learing Rate: 1.00E-02\n",
      "Epoch: 80: Training Loss: 0.069, Validation Loss: 0.082, Learing Rate: 1.00E-02\n",
      "Epoch: 81: Training Loss: 0.069, Validation Loss: 0.082, Learing Rate: 1.00E-02\n",
      "Epoch: 82: Training Loss: 0.068, Validation Loss: 0.081, Learing Rate: 1.00E-02\n",
      "Epoch: 83: Training Loss: 0.068, Validation Loss: 0.081, Learing Rate: 1.00E-02\n",
      "Epoch: 84: Training Loss: 0.067, Validation Loss: 0.080, Learing Rate: 1.00E-02\n",
      "Epoch: 85: Training Loss: 0.066, Validation Loss: 0.080, Learing Rate: 1.00E-02\n",
      "Epoch: 86: Training Loss: 0.066, Validation Loss: 0.080, Learing Rate: 1.00E-02\n",
      "Epoch: 87: Training Loss: 0.065, Validation Loss: 0.079, Learing Rate: 1.00E-02\n",
      "Epoch: 88: Training Loss: 0.065, Validation Loss: 0.079, Learing Rate: 1.00E-02\n",
      "Epoch: 89: Training Loss: 0.064, Validation Loss: 0.079, Learing Rate: 1.00E-02\n",
      "Epoch: 90: Training Loss: 0.064, Validation Loss: 0.078, Learing Rate: 1.00E-02\n",
      "Epoch: 91: Training Loss: 0.063, Validation Loss: 0.078, Learing Rate: 1.00E-02\n",
      "Epoch: 92: Training Loss: 0.063, Validation Loss: 0.077, Learing Rate: 1.00E-02\n",
      "Epoch: 93: Training Loss: 0.062, Validation Loss: 0.077, Learing Rate: 1.00E-02\n",
      "Epoch: 94: Training Loss: 0.062, Validation Loss: 0.077, Learing Rate: 1.00E-02\n",
      "Epoch: 95: Training Loss: 0.061, Validation Loss: 0.076, Learing Rate: 1.00E-02\n",
      "Epoch: 96: Training Loss: 0.061, Validation Loss: 0.076, Learing Rate: 1.00E-02\n",
      "Epoch: 97: Training Loss: 0.060, Validation Loss: 0.076, Learing Rate: 1.00E-02\n",
      "Epoch: 98: Training Loss: 0.060, Validation Loss: 0.075, Learing Rate: 1.00E-02\n",
      "Epoch: 99: Training Loss: 0.059, Validation Loss: 0.075, Learing Rate: 1.00E-02\n",
      "Epoch: 100: Training Loss: 0.059, Validation Loss: 0.075, Learing Rate: 1.00E-02\n",
      "Epoch: 101: Training Loss: 0.058, Validation Loss: 0.075, Learing Rate: 1.00E-02\n",
      "Epoch: 102: Training Loss: 0.058, Validation Loss: 0.074, Learing Rate: 1.00E-02\n",
      "Epoch: 103: Training Loss: 0.058, Validation Loss: 0.074, Learing Rate: 1.00E-02\n",
      "Epoch: 104: Training Loss: 0.057, Validation Loss: 0.074, Learing Rate: 1.00E-02\n",
      "Epoch: 105: Training Loss: 0.057, Validation Loss: 0.073, Learing Rate: 1.00E-02\n",
      "Epoch: 106: Training Loss: 0.056, Validation Loss: 0.073, Learing Rate: 1.00E-02\n",
      "Epoch: 107: Training Loss: 0.056, Validation Loss: 0.073, Learing Rate: 1.00E-02\n",
      "Epoch: 108: Training Loss: 0.056, Validation Loss: 0.072, Learing Rate: 1.00E-02\n",
      "Epoch: 109: Training Loss: 0.055, Validation Loss: 0.072, Learing Rate: 1.00E-02\n",
      "Epoch: 110: Training Loss: 0.055, Validation Loss: 0.072, Learing Rate: 1.00E-02\n",
      "Epoch: 111: Training Loss: 0.055, Validation Loss: 0.071, Learing Rate: 1.00E-02\n",
      "Epoch: 112: Training Loss: 0.054, Validation Loss: 0.071, Learing Rate: 1.00E-02\n",
      "Epoch: 113: Training Loss: 0.054, Validation Loss: 0.071, Learing Rate: 1.00E-02\n",
      "Epoch: 114: Training Loss: 0.053, Validation Loss: 0.071, Learing Rate: 1.00E-02\n",
      "Epoch: 115: Training Loss: 0.053, Validation Loss: 0.070, Learing Rate: 1.00E-02\n",
      "Epoch: 116: Training Loss: 0.053, Validation Loss: 0.070, Learing Rate: 1.00E-02\n",
      "Epoch: 117: Training Loss: 0.052, Validation Loss: 0.070, Learing Rate: 1.00E-02\n",
      "Epoch: 118: Training Loss: 0.052, Validation Loss: 0.069, Learing Rate: 1.00E-02\n",
      "Epoch: 119: Training Loss: 0.052, Validation Loss: 0.069, Learing Rate: 1.00E-02\n",
      "Epoch: 120: Training Loss: 0.051, Validation Loss: 0.069, Learing Rate: 1.00E-02\n",
      "Epoch: 121: Training Loss: 0.051, Validation Loss: 0.069, Learing Rate: 1.00E-02\n",
      "Epoch: 122: Training Loss: 0.051, Validation Loss: 0.068, Learing Rate: 1.00E-02\n",
      "Epoch: 123: Training Loss: 0.050, Validation Loss: 0.068, Learing Rate: 1.00E-02\n",
      "Epoch: 124: Training Loss: 0.050, Validation Loss: 0.068, Learing Rate: 1.00E-02\n",
      "Epoch: 125: Training Loss: 0.050, Validation Loss: 0.067, Learing Rate: 1.00E-02\n",
      "Epoch: 126: Training Loss: 0.049, Validation Loss: 0.067, Learing Rate: 1.00E-02\n",
      "Epoch: 127: Training Loss: 0.049, Validation Loss: 0.067, Learing Rate: 1.00E-02\n",
      "Epoch: 128: Training Loss: 0.049, Validation Loss: 0.067, Learing Rate: 1.00E-02\n",
      "Epoch: 129: Training Loss: 0.049, Validation Loss: 0.066, Learing Rate: 1.00E-02\n",
      "Epoch: 130: Training Loss: 0.048, Validation Loss: 0.066, Learing Rate: 1.00E-02\n",
      "Epoch: 131: Training Loss: 0.048, Validation Loss: 0.066, Learing Rate: 1.00E-02\n",
      "Epoch: 132: Training Loss: 0.048, Validation Loss: 0.066, Learing Rate: 1.00E-02\n",
      "Epoch: 133: Training Loss: 0.047, Validation Loss: 0.066, Learing Rate: 1.00E-02\n",
      "Epoch: 134: Training Loss: 0.047, Validation Loss: 0.065, Learing Rate: 1.00E-02\n",
      "Epoch: 135: Training Loss: 0.047, Validation Loss: 0.065, Learing Rate: 1.00E-02\n",
      "Epoch: 136: Training Loss: 0.047, Validation Loss: 0.065, Learing Rate: 1.00E-02\n",
      "Epoch: 137: Training Loss: 0.046, Validation Loss: 0.065, Learing Rate: 1.00E-02\n",
      "Epoch: 138: Training Loss: 0.046, Validation Loss: 0.064, Learing Rate: 1.00E-02\n",
      "Epoch: 139: Training Loss: 0.046, Validation Loss: 0.064, Learing Rate: 1.00E-02\n",
      "Epoch: 140: Training Loss: 0.045, Validation Loss: 0.064, Learing Rate: 1.00E-02\n",
      "Epoch: 141: Training Loss: 0.045, Validation Loss: 0.064, Learing Rate: 1.00E-02\n",
      "Epoch: 142: Training Loss: 0.045, Validation Loss: 0.064, Learing Rate: 1.00E-02\n",
      "Epoch: 143: Training Loss: 0.045, Validation Loss: 0.063, Learing Rate: 1.00E-02\n",
      "Epoch: 144: Training Loss: 0.044, Validation Loss: 0.063, Learing Rate: 1.00E-02\n",
      "Epoch: 145: Training Loss: 0.044, Validation Loss: 0.063, Learing Rate: 1.00E-02\n",
      "Epoch: 146: Training Loss: 0.044, Validation Loss: 0.063, Learing Rate: 1.00E-02\n",
      "Epoch: 147: Training Loss: 0.044, Validation Loss: 0.063, Learing Rate: 1.00E-02\n",
      "Epoch: 148: Training Loss: 0.043, Validation Loss: 0.062, Learing Rate: 1.00E-02\n",
      "Epoch: 149: Training Loss: 0.043, Validation Loss: 0.062, Learing Rate: 1.00E-02\n",
      "Epoch: 150: Training Loss: 0.043, Validation Loss: 0.062, Learing Rate: 1.00E-02\n",
      "Epoch: 151: Training Loss: 0.043, Validation Loss: 0.062, Learing Rate: 1.00E-02\n",
      "Epoch: 152: Training Loss: 0.043, Validation Loss: 0.062, Learing Rate: 1.00E-02\n",
      "Epoch: 153: Training Loss: 0.042, Validation Loss: 0.061, Learing Rate: 1.00E-02\n",
      "Epoch: 154: Training Loss: 0.042, Validation Loss: 0.061, Learing Rate: 1.00E-02\n",
      "Epoch: 155: Training Loss: 0.042, Validation Loss: 0.061, Learing Rate: 1.00E-02\n",
      "Epoch: 156: Training Loss: 0.042, Validation Loss: 0.061, Learing Rate: 1.00E-02\n",
      "Epoch: 157: Training Loss: 0.041, Validation Loss: 0.061, Learing Rate: 1.00E-02\n",
      "Epoch: 158: Training Loss: 0.041, Validation Loss: 0.060, Learing Rate: 1.00E-02\n",
      "Epoch: 159: Training Loss: 0.041, Validation Loss: 0.060, Learing Rate: 1.00E-02\n",
      "Epoch: 160: Training Loss: 0.041, Validation Loss: 0.060, Learing Rate: 1.00E-02\n",
      "Epoch: 161: Training Loss: 0.040, Validation Loss: 0.060, Learing Rate: 1.00E-02\n",
      "Epoch: 162: Training Loss: 0.040, Validation Loss: 0.060, Learing Rate: 1.00E-02\n",
      "Epoch: 163: Training Loss: 0.040, Validation Loss: 0.060, Learing Rate: 1.00E-02\n",
      "Epoch: 164: Training Loss: 0.040, Validation Loss: 0.059, Learing Rate: 1.00E-02\n",
      "Epoch: 165: Training Loss: 0.040, Validation Loss: 0.059, Learing Rate: 1.00E-02\n",
      "Epoch: 166: Training Loss: 0.039, Validation Loss: 0.059, Learing Rate: 1.00E-02\n",
      "Epoch: 167: Training Loss: 0.039, Validation Loss: 0.059, Learing Rate: 1.00E-02\n",
      "Epoch: 168: Training Loss: 0.039, Validation Loss: 0.059, Learing Rate: 1.00E-02\n",
      "Epoch: 169: Training Loss: 0.039, Validation Loss: 0.059, Learing Rate: 1.00E-02\n",
      "Epoch: 170: Training Loss: 0.039, Validation Loss: 0.059, Learing Rate: 1.00E-02\n",
      "Epoch: 171: Training Loss: 0.038, Validation Loss: 0.058, Learing Rate: 1.00E-02\n",
      "Epoch: 172: Training Loss: 0.038, Validation Loss: 0.058, Learing Rate: 1.00E-02\n",
      "Epoch: 173: Training Loss: 0.038, Validation Loss: 0.058, Learing Rate: 1.00E-02\n",
      "Epoch: 174: Training Loss: 0.038, Validation Loss: 0.058, Learing Rate: 1.00E-02\n",
      "Epoch: 175: Training Loss: 0.038, Validation Loss: 0.058, Learing Rate: 1.00E-02\n",
      "Epoch: 176: Training Loss: 0.037, Validation Loss: 0.058, Learing Rate: 1.00E-02\n",
      "Epoch: 177: Training Loss: 0.037, Validation Loss: 0.058, Learing Rate: 1.00E-02\n",
      "Epoch: 178: Training Loss: 0.037, Validation Loss: 0.057, Learing Rate: 1.00E-02\n",
      "Epoch: 179: Training Loss: 0.037, Validation Loss: 0.057, Learing Rate: 1.00E-02\n",
      "Epoch: 180: Training Loss: 0.037, Validation Loss: 0.057, Learing Rate: 1.00E-02\n",
      "Epoch: 181: Training Loss: 0.036, Validation Loss: 0.057, Learing Rate: 1.00E-02\n",
      "Epoch: 182: Training Loss: 0.036, Validation Loss: 0.057, Learing Rate: 1.00E-02\n",
      "Epoch: 183: Training Loss: 0.036, Validation Loss: 0.057, Learing Rate: 1.00E-02\n",
      "Epoch: 184: Training Loss: 0.036, Validation Loss: 0.057, Learing Rate: 1.00E-02\n",
      "Epoch: 185: Training Loss: 0.036, Validation Loss: 0.057, Learing Rate: 1.00E-02\n",
      "Epoch: 186: Training Loss: 0.036, Validation Loss: 0.057, Learing Rate: 1.00E-02\n",
      "Epoch: 187: Training Loss: 0.035, Validation Loss: 0.056, Learing Rate: 1.00E-02\n",
      "Epoch: 188: Training Loss: 0.035, Validation Loss: 0.056, Learing Rate: 1.00E-02\n",
      "Epoch: 189: Training Loss: 0.035, Validation Loss: 0.056, Learing Rate: 1.00E-02\n",
      "Epoch: 190: Training Loss: 0.035, Validation Loss: 0.056, Learing Rate: 1.00E-02\n",
      "Epoch: 191: Training Loss: 0.035, Validation Loss: 0.056, Learing Rate: 1.00E-02\n",
      "Epoch: 192: Training Loss: 0.035, Validation Loss: 0.056, Learing Rate: 1.00E-02\n",
      "Epoch: 193: Training Loss: 0.034, Validation Loss: 0.056, Learing Rate: 1.00E-02\n",
      "Epoch: 194: Training Loss: 0.034, Validation Loss: 0.056, Learing Rate: 1.00E-02\n",
      "Epoch: 195: Training Loss: 0.034, Validation Loss: 0.056, Learing Rate: 1.00E-02\n",
      "Epoch: 196: Training Loss: 0.034, Validation Loss: 0.056, Learing Rate: 1.00E-02\n",
      "Epoch: 197: Training Loss: 0.034, Validation Loss: 0.055, Learing Rate: 1.00E-02\n",
      "Epoch: 198: Training Loss: 0.034, Validation Loss: 0.055, Learing Rate: 1.00E-02\n",
      "Epoch: 199: Training Loss: 0.033, Validation Loss: 0.055, Learing Rate: 1.00E-02\n",
      "Epoch: 200: Training Loss: 0.033, Validation Loss: 0.055, Learing Rate: 1.00E-02\n",
      "Epoch: 201: Training Loss: 0.033, Validation Loss: 0.055, Learing Rate: 1.00E-02\n",
      "Epoch: 202: Training Loss: 0.033, Validation Loss: 0.055, Learing Rate: 1.00E-02\n",
      "Epoch: 203: Training Loss: 0.033, Validation Loss: 0.055, Learing Rate: 1.00E-02\n",
      "Epoch: 204: Training Loss: 0.033, Validation Loss: 0.055, Learing Rate: 1.00E-02\n",
      "Epoch: 205: Training Loss: 0.032, Validation Loss: 0.055, Learing Rate: 1.00E-02\n",
      "Epoch: 206: Training Loss: 0.032, Validation Loss: 0.055, Learing Rate: 1.00E-02\n",
      "Epoch: 207: Training Loss: 0.032, Validation Loss: 0.055, Learing Rate: 1.00E-02\n",
      "Epoch: 208: Training Loss: 0.032, Validation Loss: 0.054, Learing Rate: 1.00E-02\n",
      "Epoch: 209: Training Loss: 0.032, Validation Loss: 0.054, Learing Rate: 1.00E-02\n",
      "Epoch: 210: Training Loss: 0.032, Validation Loss: 0.054, Learing Rate: 1.00E-02\n",
      "Epoch: 211: Training Loss: 0.032, Validation Loss: 0.054, Learing Rate: 1.00E-02\n",
      "Epoch: 212: Training Loss: 0.031, Validation Loss: 0.054, Learing Rate: 1.00E-02\n",
      "Epoch: 213: Training Loss: 0.031, Validation Loss: 0.054, Learing Rate: 1.00E-02\n",
      "Epoch: 214: Training Loss: 0.031, Validation Loss: 0.054, Learing Rate: 1.00E-02\n",
      "Epoch: 215: Training Loss: 0.031, Validation Loss: 0.054, Learing Rate: 1.00E-02\n",
      "Epoch: 216: Training Loss: 0.031, Validation Loss: 0.054, Learing Rate: 1.00E-02\n",
      "Epoch: 217: Training Loss: 0.031, Validation Loss: 0.054, Learing Rate: 1.00E-02\n",
      "Epoch: 218: Training Loss: 0.031, Validation Loss: 0.054, Learing Rate: 1.00E-02\n",
      "Epoch: 219: Training Loss: 0.030, Validation Loss: 0.054, Learing Rate: 1.00E-02\n",
      "Epoch: 220: Training Loss: 0.030, Validation Loss: 0.054, Learing Rate: 1.00E-02\n",
      "Epoch: 221: Training Loss: 0.030, Validation Loss: 0.054, Learing Rate: 1.00E-02\n",
      "Epoch: 222: Training Loss: 0.030, Validation Loss: 0.054, Learing Rate: 1.00E-02\n",
      "Epoch: 223: Training Loss: 0.030, Validation Loss: 0.053, Learing Rate: 1.00E-02\n",
      "Epoch: 224: Training Loss: 0.030, Validation Loss: 0.053, Learing Rate: 1.00E-02\n",
      "Epoch: 225: Training Loss: 0.030, Validation Loss: 0.053, Learing Rate: 1.00E-02\n",
      "Epoch: 226: Training Loss: 0.029, Validation Loss: 0.053, Learing Rate: 1.00E-02\n",
      "Epoch: 227: Training Loss: 0.029, Validation Loss: 0.053, Learing Rate: 1.00E-02\n",
      "Epoch: 228: Training Loss: 0.029, Validation Loss: 0.053, Learing Rate: 1.00E-02\n",
      "Epoch: 229: Training Loss: 0.029, Validation Loss: 0.053, Learing Rate: 1.00E-02\n",
      "Epoch: 230: Training Loss: 0.029, Validation Loss: 0.053, Learing Rate: 1.00E-02\n",
      "Epoch: 231: Training Loss: 0.029, Validation Loss: 0.053, Learing Rate: 1.00E-02\n",
      "Epoch: 232: Training Loss: 0.029, Validation Loss: 0.053, Learing Rate: 1.00E-02\n",
      "Epoch: 233: Training Loss: 0.029, Validation Loss: 0.053, Learing Rate: 1.00E-02\n",
      "Epoch: 234: Training Loss: 0.028, Validation Loss: 0.053, Learing Rate: 1.00E-02\n",
      "Epoch: 235: Training Loss: 0.028, Validation Loss: 0.053, Learing Rate: 1.00E-02\n",
      "Epoch: 236: Training Loss: 0.028, Validation Loss: 0.053, Learing Rate: 1.00E-02\n",
      "Epoch: 237: Training Loss: 0.028, Validation Loss: 0.053, Learing Rate: 1.00E-02\n",
      "Epoch: 238: Training Loss: 0.028, Validation Loss: 0.053, Learing Rate: 1.00E-02\n",
      "Epoch: 239: Training Loss: 0.028, Validation Loss: 0.053, Learing Rate: 1.00E-02\n",
      "Epoch: 240: Training Loss: 0.028, Validation Loss: 0.053, Learing Rate: 1.00E-02\n",
      "Epoch: 241: Training Loss: 0.028, Validation Loss: 0.053, Learing Rate: 1.00E-02\n",
      "Epoch: 242: Training Loss: 0.027, Validation Loss: 0.053, Learing Rate: 1.00E-02\n",
      "Epoch: 243: Training Loss: 0.027, Validation Loss: 0.053, Learing Rate: 1.00E-02\n",
      "Epoch: 244: Training Loss: 0.027, Validation Loss: 0.052, Learing Rate: 1.00E-02\n",
      "Epoch: 245: Training Loss: 0.027, Validation Loss: 0.052, Learing Rate: 1.00E-02\n",
      "Epoch: 246: Training Loss: 0.027, Validation Loss: 0.052, Learing Rate: 1.00E-02\n",
      "Epoch: 247: Training Loss: 0.027, Validation Loss: 0.052, Learing Rate: 1.00E-02\n",
      "Epoch: 248: Training Loss: 0.027, Validation Loss: 0.052, Learing Rate: 1.00E-02\n",
      "Epoch: 249: Training Loss: 0.027, Validation Loss: 0.052, Learing Rate: 1.00E-02\n",
      "Epoch: 250: Training Loss: 0.026, Validation Loss: 0.052, Learing Rate: 1.00E-02\n",
      "Epoch: 251: Training Loss: 0.026, Validation Loss: 0.052, Learing Rate: 1.00E-02\n",
      "Epoch: 252: Training Loss: 0.026, Validation Loss: 0.052, Learing Rate: 1.00E-02\n",
      "Epoch: 253: Training Loss: 0.026, Validation Loss: 0.052, Learing Rate: 1.00E-02\n",
      "Epoch: 254: Training Loss: 0.026, Validation Loss: 0.052, Learing Rate: 1.00E-02\n",
      "Epoch: 255: Training Loss: 0.026, Validation Loss: 0.052, Learing Rate: 1.00E-02\n",
      "Epoch: 256: Training Loss: 0.026, Validation Loss: 0.052, Learing Rate: 1.00E-02\n",
      "Epoch: 257: Training Loss: 0.026, Validation Loss: 0.052, Learing Rate: 1.00E-02\n",
      "Epoch: 258: Training Loss: 0.026, Validation Loss: 0.052, Learing Rate: 1.00E-02\n",
      "Epoch: 259: Training Loss: 0.025, Validation Loss: 0.052, Learing Rate: 1.00E-02\n",
      "Epoch: 260: Training Loss: 0.025, Validation Loss: 0.052, Learing Rate: 1.00E-02\n",
      "Epoch: 261: Training Loss: 0.025, Validation Loss: 0.052, Learing Rate: 1.00E-02\n",
      "Epoch: 262: Training Loss: 0.025, Validation Loss: 0.052, Learing Rate: 1.00E-02\n",
      "Epoch: 263: Training Loss: 0.025, Validation Loss: 0.052, Learing Rate: 5.00E-03\n",
      "Epoch: 264: Training Loss: 0.024, Validation Loss: 0.050, Learing Rate: 5.00E-03\n",
      "Epoch: 265: Training Loss: 0.024, Validation Loss: 0.050, Learing Rate: 5.00E-03\n",
      "Epoch: 266: Training Loss: 0.024, Validation Loss: 0.050, Learing Rate: 5.00E-03\n",
      "Epoch: 267: Training Loss: 0.024, Validation Loss: 0.050, Learing Rate: 5.00E-03\n",
      "Epoch: 268: Training Loss: 0.024, Validation Loss: 0.050, Learing Rate: 5.00E-03\n",
      "Epoch: 269: Training Loss: 0.024, Validation Loss: 0.050, Learing Rate: 5.00E-03\n",
      "Epoch: 270: Training Loss: 0.024, Validation Loss: 0.050, Learing Rate: 5.00E-03\n",
      "Epoch: 271: Training Loss: 0.023, Validation Loss: 0.050, Learing Rate: 5.00E-03\n",
      "Epoch: 272: Training Loss: 0.023, Validation Loss: 0.050, Learing Rate: 5.00E-03\n",
      "Epoch: 273: Training Loss: 0.023, Validation Loss: 0.050, Learing Rate: 5.00E-03\n",
      "Epoch: 274: Training Loss: 0.023, Validation Loss: 0.050, Learing Rate: 2.50E-03\n",
      "Epoch: 275: Training Loss: 0.023, Validation Loss: 0.048, Learing Rate: 2.50E-03\n",
      "Epoch: 276: Training Loss: 0.023, Validation Loss: 0.048, Learing Rate: 2.50E-03\n",
      "Epoch: 277: Training Loss: 0.023, Validation Loss: 0.048, Learing Rate: 2.50E-03\n",
      "Epoch: 278: Training Loss: 0.023, Validation Loss: 0.048, Learing Rate: 2.50E-03\n",
      "Epoch: 279: Training Loss: 0.023, Validation Loss: 0.048, Learing Rate: 2.50E-03\n",
      "Epoch: 280: Training Loss: 0.023, Validation Loss: 0.048, Learing Rate: 2.50E-03\n",
      "Epoch: 281: Training Loss: 0.023, Validation Loss: 0.048, Learing Rate: 2.50E-03\n",
      "Epoch: 282: Training Loss: 0.023, Validation Loss: 0.048, Learing Rate: 2.50E-03\n",
      "Epoch: 283: Training Loss: 0.023, Validation Loss: 0.048, Learing Rate: 2.50E-03\n",
      "Epoch: 284: Training Loss: 0.023, Validation Loss: 0.048, Learing Rate: 2.50E-03\n",
      "Epoch: 285: Training Loss: 0.023, Validation Loss: 0.048, Learing Rate: 1.25E-03\n",
      "Epoch: 286: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.25E-03\n",
      "Epoch: 287: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.25E-03\n",
      "Epoch: 288: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.25E-03\n",
      "Epoch: 289: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.25E-03\n",
      "Epoch: 290: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.25E-03\n",
      "Epoch: 291: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.25E-03\n",
      "Epoch: 292: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.25E-03\n",
      "Epoch: 293: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.25E-03\n",
      "Epoch: 294: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.25E-03\n",
      "Epoch: 295: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.25E-03\n",
      "Epoch: 296: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 6.25E-04\n",
      "Epoch: 297: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 6.25E-04\n",
      "Epoch: 298: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 6.25E-04\n",
      "Epoch: 299: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 6.25E-04\n",
      "Epoch: 300: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 6.25E-04\n",
      "Epoch: 301: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 6.25E-04\n",
      "Epoch: 302: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 6.25E-04\n",
      "Epoch: 303: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 6.25E-04\n",
      "Epoch: 304: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 6.25E-04\n",
      "Epoch: 305: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 6.25E-04\n",
      "Epoch: 306: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 6.25E-04\n",
      "Epoch: 307: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.13E-04\n",
      "Epoch: 308: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.13E-04\n",
      "Epoch: 309: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.13E-04\n",
      "Epoch: 310: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.13E-04\n",
      "Epoch: 311: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.13E-04\n",
      "Epoch: 312: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.13E-04\n",
      "Epoch: 313: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.13E-04\n",
      "Epoch: 314: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.13E-04\n",
      "Epoch: 315: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.13E-04\n",
      "Epoch: 316: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.13E-04\n",
      "Epoch: 317: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.13E-04\n",
      "Epoch: 318: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.56E-04\n",
      "Epoch: 319: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.56E-04\n",
      "Epoch: 320: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.56E-04\n",
      "Epoch: 321: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.56E-04\n",
      "Epoch: 322: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.56E-04\n",
      "Epoch: 323: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.56E-04\n",
      "Epoch: 324: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.56E-04\n",
      "Epoch: 325: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.56E-04\n",
      "Epoch: 326: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.56E-04\n",
      "Epoch: 327: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.56E-04\n",
      "Epoch: 328: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.56E-04\n",
      "Epoch: 329: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 7.81E-05\n",
      "Epoch: 330: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 7.81E-05\n",
      "Epoch: 331: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 7.81E-05\n",
      "Epoch: 332: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 7.81E-05\n",
      "Epoch: 333: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 7.81E-05\n",
      "Epoch: 334: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 7.81E-05\n",
      "Epoch: 335: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 7.81E-05\n",
      "Epoch: 336: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 7.81E-05\n",
      "Epoch: 337: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 7.81E-05\n",
      "Epoch: 338: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 7.81E-05\n",
      "Epoch: 339: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 7.81E-05\n",
      "Epoch: 340: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.91E-05\n",
      "Epoch: 341: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.91E-05\n",
      "Epoch: 342: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.91E-05\n",
      "Epoch: 343: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.91E-05\n",
      "Epoch: 344: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.91E-05\n",
      "Epoch: 345: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.91E-05\n",
      "Epoch: 346: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.91E-05\n",
      "Epoch: 347: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.91E-05\n",
      "Epoch: 348: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.91E-05\n",
      "Epoch: 349: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.91E-05\n",
      "Epoch: 350: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.91E-05\n",
      "Epoch: 351: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.95E-05\n",
      "Epoch: 352: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.95E-05\n",
      "Epoch: 353: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.95E-05\n",
      "Epoch: 354: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.95E-05\n",
      "Epoch: 355: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.95E-05\n",
      "Epoch: 356: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.95E-05\n",
      "Epoch: 357: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.95E-05\n",
      "Epoch: 358: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.95E-05\n",
      "Epoch: 359: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.95E-05\n",
      "Epoch: 360: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.95E-05\n",
      "Epoch: 361: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.95E-05\n",
      "Epoch: 362: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 9.77E-06\n",
      "Epoch: 363: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 9.77E-06\n",
      "Epoch: 364: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 9.77E-06\n",
      "Epoch: 365: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 9.77E-06\n",
      "Epoch: 366: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 9.77E-06\n",
      "Epoch: 367: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 9.77E-06\n",
      "Epoch: 368: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 9.77E-06\n",
      "Epoch: 369: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 9.77E-06\n",
      "Epoch: 370: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 9.77E-06\n",
      "Epoch: 371: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 9.77E-06\n",
      "Epoch: 372: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 9.77E-06\n",
      "Epoch: 373: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 4.88E-06\n",
      "Epoch: 374: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 4.88E-06\n",
      "Epoch: 375: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 4.88E-06\n",
      "Epoch: 376: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 4.88E-06\n",
      "Epoch: 377: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 4.88E-06\n",
      "Epoch: 378: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 4.88E-06\n",
      "Epoch: 379: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 4.88E-06\n",
      "Epoch: 380: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 4.88E-06\n",
      "Epoch: 381: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 4.88E-06\n",
      "Epoch: 382: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 4.88E-06\n",
      "Epoch: 383: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 4.88E-06\n",
      "Epoch: 384: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 2.44E-06\n",
      "Epoch: 385: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 2.44E-06\n",
      "Epoch: 386: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 2.44E-06\n",
      "Epoch: 387: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 2.44E-06\n",
      "Epoch: 388: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 2.44E-06\n",
      "Epoch: 389: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 2.44E-06\n",
      "Epoch: 390: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 2.44E-06\n",
      "Epoch: 391: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 2.44E-06\n",
      "Epoch: 392: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 2.44E-06\n",
      "Epoch: 393: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 2.44E-06\n",
      "Epoch: 394: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 2.44E-06\n",
      "Epoch: 395: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.22E-06\n",
      "Epoch: 396: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.22E-06\n",
      "Epoch: 397: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.22E-06\n",
      "Epoch: 398: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.22E-06\n",
      "Epoch: 399: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.22E-06\n",
      "Epoch: 400: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.22E-06\n",
      "Epoch: 401: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.22E-06\n",
      "Epoch: 402: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.22E-06\n",
      "Epoch: 403: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.22E-06\n",
      "Epoch: 404: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.22E-06\n",
      "Epoch: 405: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.22E-06\n",
      "Epoch: 406: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 6.10E-07\n",
      "Epoch: 407: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 6.10E-07\n",
      "Epoch: 408: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 6.10E-07\n",
      "Epoch: 409: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 6.10E-07\n",
      "Epoch: 410: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 6.10E-07\n",
      "Epoch: 411: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 6.10E-07\n",
      "Epoch: 412: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 6.10E-07\n",
      "Epoch: 413: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 6.10E-07\n",
      "Epoch: 414: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 6.10E-07\n",
      "Epoch: 415: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 6.10E-07\n",
      "Epoch: 416: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 6.10E-07\n",
      "Epoch: 417: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.05E-07\n",
      "Epoch: 418: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.05E-07\n",
      "Epoch: 419: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.05E-07\n",
      "Epoch: 420: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.05E-07\n",
      "Epoch: 421: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.05E-07\n",
      "Epoch: 422: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.05E-07\n",
      "Epoch: 423: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.05E-07\n",
      "Epoch: 424: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.05E-07\n",
      "Epoch: 425: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.05E-07\n",
      "Epoch: 426: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.05E-07\n",
      "Epoch: 427: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.05E-07\n",
      "Epoch: 428: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.53E-07\n",
      "Epoch: 429: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.53E-07\n",
      "Epoch: 430: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.53E-07\n",
      "Epoch: 431: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.53E-07\n",
      "Epoch: 432: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.53E-07\n",
      "Epoch: 433: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.53E-07\n",
      "Epoch: 434: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.53E-07\n",
      "Epoch: 435: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.53E-07\n",
      "Epoch: 436: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.53E-07\n",
      "Epoch: 437: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.53E-07\n",
      "Epoch: 438: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.53E-07\n",
      "Epoch: 439: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 7.63E-08\n",
      "Epoch: 440: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 7.63E-08\n",
      "Epoch: 441: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 7.63E-08\n",
      "Epoch: 442: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 7.63E-08\n",
      "Epoch: 443: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 7.63E-08\n",
      "Epoch: 444: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 7.63E-08\n",
      "Epoch: 445: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 7.63E-08\n",
      "Epoch: 446: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 7.63E-08\n",
      "Epoch: 447: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 7.63E-08\n",
      "Epoch: 448: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 7.63E-08\n",
      "Epoch: 449: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 7.63E-08\n",
      "Epoch: 450: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.81E-08\n",
      "Epoch: 451: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.81E-08\n",
      "Epoch: 452: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.81E-08\n",
      "Epoch: 453: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.81E-08\n",
      "Epoch: 454: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.81E-08\n",
      "Epoch: 455: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.81E-08\n",
      "Epoch: 456: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.81E-08\n",
      "Epoch: 457: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.81E-08\n",
      "Epoch: 458: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.81E-08\n",
      "Epoch: 459: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.81E-08\n",
      "Epoch: 460: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 3.81E-08\n",
      "Epoch: 461: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 462: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 463: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 464: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 465: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 466: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 467: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 468: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 469: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 470: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 471: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 472: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 473: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 474: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 475: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 476: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 477: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 478: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 479: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 480: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 481: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 482: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 483: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 484: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 485: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 486: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 487: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 488: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 489: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 490: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 491: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 492: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 493: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 494: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 495: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 496: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 497: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 498: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 499: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 500: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 501: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 502: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 503: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 504: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 505: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 506: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 507: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 508: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 509: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 510: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 511: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 512: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 513: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 514: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 515: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 516: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 517: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 518: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 519: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 520: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 521: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 522: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 523: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 524: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 525: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 526: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 527: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 528: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 529: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 530: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 531: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 532: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 533: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 534: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 535: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 536: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 537: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 538: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 539: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 540: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 541: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 542: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 543: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 544: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 545: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 546: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 547: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 548: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 549: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 550: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 551: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 552: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 553: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 554: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 555: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 556: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 557: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 558: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 559: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 560: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 561: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 562: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 563: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 564: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 565: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 566: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 567: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 568: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 569: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 570: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 571: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 572: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 573: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 574: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 575: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 576: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 577: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 578: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 579: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 580: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 581: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 582: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 583: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 584: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 585: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 586: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 587: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 588: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 589: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 590: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 591: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 592: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 593: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 594: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 595: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 596: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 597: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 598: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 599: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 600: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 601: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 602: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 603: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 604: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 605: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 606: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 607: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 608: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 609: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 610: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 611: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 612: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 613: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 614: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 615: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 616: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 617: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 618: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 619: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 620: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 621: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 622: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 623: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 624: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 625: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 626: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 627: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 628: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 629: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 630: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 631: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 632: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 633: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 634: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 635: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 636: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 637: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 638: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 639: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 640: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 641: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 642: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 643: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 644: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 645: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 646: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 647: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 648: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 649: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 650: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 651: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 652: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 653: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 654: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 655: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 656: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 657: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 658: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 659: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 660: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 661: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 662: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 663: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 664: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 665: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 666: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 667: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 668: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 669: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 670: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 671: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 672: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 673: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 674: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 675: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 676: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 677: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 678: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 679: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 680: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 681: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 682: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 683: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 684: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 685: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 686: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 687: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 688: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 689: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 690: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 691: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 692: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 693: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 694: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 695: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 696: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 697: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 698: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 699: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 700: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 701: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 702: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 703: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 704: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 705: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 706: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 707: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 708: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 709: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 710: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 711: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 712: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 713: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 714: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 715: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 716: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 717: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 718: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 719: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 720: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 721: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 722: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 723: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 724: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 725: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 726: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 727: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 728: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 729: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 730: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 731: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 732: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 733: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 734: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 735: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 736: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 737: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 738: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 739: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 740: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 741: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 742: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 743: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 744: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 745: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 746: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 747: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 748: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 749: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 750: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 751: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 752: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 753: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 754: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 755: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 756: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 757: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 758: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 759: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 760: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 761: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 762: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 763: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 764: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 765: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 766: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 767: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 768: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 769: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 770: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 771: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 772: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 773: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 774: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 775: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 776: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 777: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 778: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 779: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 780: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 781: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 782: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 783: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 784: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 785: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 786: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 787: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 788: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 789: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 790: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 791: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 792: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 793: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 794: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 795: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 796: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 797: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 798: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 799: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 800: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 801: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 802: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 803: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 804: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 805: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 806: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 807: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 808: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 809: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 810: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 811: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 812: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 813: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 814: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 815: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 816: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 817: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 818: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 819: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 820: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 821: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 822: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 823: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 824: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 825: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 826: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 827: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 828: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 829: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 830: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 831: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 832: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 833: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 834: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 835: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 836: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 837: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 838: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 839: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 840: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 841: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 842: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 843: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 844: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 845: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 846: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 847: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 848: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 849: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 850: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 851: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 852: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 853: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 854: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 855: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 856: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 857: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 858: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 859: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 860: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 861: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 862: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 863: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 864: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 865: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 866: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 867: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 868: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 869: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 870: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 871: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 872: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 873: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 874: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 875: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 876: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 877: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 878: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 879: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 880: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 881: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 882: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 883: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 884: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 885: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 886: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 887: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 888: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 889: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 890: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 891: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 892: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 893: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 894: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 895: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 896: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 897: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 898: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 899: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 900: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 901: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 902: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 903: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 904: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 905: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 906: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 907: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 908: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 909: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 910: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 911: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 912: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 913: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 914: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 915: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 916: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 917: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 918: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 919: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 920: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 921: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 922: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 923: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 924: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 925: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 926: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 927: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 928: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 929: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 930: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 931: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 932: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 933: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 934: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 935: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 936: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 937: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 938: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 939: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 940: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 941: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 942: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 943: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 944: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 945: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 946: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 947: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 948: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 949: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 950: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 951: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 952: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 953: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 954: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 955: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 956: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 957: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 958: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 959: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 960: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 961: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 962: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 963: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 964: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 965: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 966: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 967: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 968: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 969: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 970: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 971: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 972: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 973: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 974: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 975: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 976: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 977: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 978: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 979: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 980: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 981: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 982: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 983: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 984: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 985: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 986: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 987: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 988: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 989: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 990: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 991: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 992: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 993: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 994: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 995: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 996: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 997: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 998: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n",
      "Epoch: 999: Training Loss: 0.022, Validation Loss: 0.047, Learing Rate: 1.91E-08\n"
     ]
    }
   ],
   "source": [
    "# Training and validation\n",
    "\n",
    "# Loss and optimization selection\n",
    "model = Basic_CNN_MNISTClassifer()\n",
    "loss_fnc = torch.nn.functional.nll_loss\n",
    "lr = 1e-2\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Training scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5)\n",
    "\n",
    "# Move model to parallel GPU (all cores)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# model= nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "# model = model.module.to(device)\n",
    "\n",
    "# Set epoch number and track losses\n",
    "num_epochs = 1000\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "lrs = [lr]\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        # Forward batch\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = loss_fnc(logits, y)\n",
    "        train_loss += loss\n",
    "\n",
    "        # Backprop and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    train_losses.append(train_loss/len(train_loader))\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for batch in val_loader:\n",
    "            # Forward batch\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = loss_fnc(logits, y)\n",
    "            val_loss += loss\n",
    "\n",
    "        # adjust the learning rate\n",
    "        scheduler.step(loss)\n",
    "        lrs.append(scheduler.get_last_lr()[0])\n",
    "        val_losses.append(val_loss/len(val_loader))\n",
    "\n",
    "    # Update progress\n",
    "    # print(f'{epoch=}: {train_losses[-1]=}, {val_losses[-1]=}')\n",
    "    print(f'Epoch: {epoch}: Training Loss: {train_losses[-1]:.3f}, Validation Loss: {val_losses[-1]:.3f}, Learing Rate: {lrs[-1]:.2E}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d3409f5c70>]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate training and validation loss curves\n",
    "train_losses = [loss.to('cpu').detach() for loss in train_losses]\n",
    "val_losses = [loss.to('cpu').detach() for loss in val_losses]\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(range(len(train_losses)), [loss for loss in train_losses])\n",
    "plt.plot(range(len(val_losses)), [loss for loss in val_losses])\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(range(len(train_losses)), [torch.log(loss) for loss in train_losses])\n",
    "plt.plot(range(len(val_losses)), [torch.log(loss) for loss in val_losses])\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(range(len(lrs)), [log(lr) for lr in lrs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34749e3b56a41a5a399427c11683855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.85%\n"
     ]
    }
   ],
   "source": [
    "# Perform inference on the test dataset\n",
    "samples, targets, predictions = [], [], []\n",
    "for batch_samples, batch_targets in tqdm(test_loader):\n",
    "    samples += [sample.squeeze() for sample in batch_samples]\n",
    "    targets += batch_targets.tolist()\n",
    "\n",
    "    model.to('cpu')\n",
    "    for sample, target in zip(batch_samples, batch_targets):\n",
    "        sample = sample.unsqueeze(dim=0)\n",
    "        output = model(sample)\n",
    "        output = torch.exp(output)\n",
    "        prediction = torch.argmax(output)\n",
    "        # target = target.item()\n",
    "        predictions.append(prediction)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = sum([target == prediction for target, prediction in zip(targets, predictions)])/len(targets)*100\n",
    "print(f'Accuracy: {accuracy.item():.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction = 7, target = 7, confidence = 58.65%\n",
      "Top 3: [7, 2, 3]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explore some example predictions from the test set\n",
    "# Search for low confidence predictions by setting <threshold_confidence>\n",
    "max_iters, curr_iters = 500, 0\n",
    "while True:\n",
    "    threshold_confidence = 60\n",
    "    batch_index = torch.randint(len(test_loader), (1,))\n",
    "    batch_samples, batch_targets = test_loader[batch_index]\n",
    "    sample_index = torch.randint(len(batch_samples), (1,))\n",
    "    img_sample = batch_samples[sample_index].squeeze()\n",
    "    sample_target = batch_targets[sample_index]\n",
    "    img_sample = img_sample\n",
    "    output = model(img_sample.unsqueeze(dim=0).unsqueeze(dim=0))\n",
    "    top3 = torch.argsort(output, descending=True).squeeze()[:3].tolist()\n",
    "    prediction = top3[0]\n",
    "    confidence = torch.max(torch.exp(output)).item()*100\n",
    "    curr_iters += 1\n",
    "    if confidence < threshold_confidence:\n",
    "        print(f'Prediction = {prediction}, target = {sample_target.item()}, confidence = {confidence:.2f}%')\n",
    "        print(f'Top 3: {top3}')\n",
    "        plt.subplot(1,2, 1)\n",
    "        plt.imshow(img_sample)\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.bar(torch.arange(len(output.squeeze())), torch.exp(output.squeeze()).detach()*100)\n",
    "        break\n",
    "    elif curr_iters < max_iters:\n",
    "        continue\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "- After model definition changes, no changes required for the training and evaluation stages\n",
    "- Accuracy of 98.85% on the test set\n",
    "- Low confidence predictions are much harder to find\n",
    "    - Max iterations on search increased from 100 to 500"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
